from keras.layers import *
from keras.models import *
from keras.optimizers import Adam
from keras.regularizers import l1, l2
from keras.engine.topology import Layer, InputSpec
from keras import backend as K
from keras import initializers
from keras import activations
from keras import optimizers, regularizers
import numpy as np
MAX_LEN_en = 3000
MAX_LEN_pr = 2000
NB_WORDS = 4097
EMBEDDING_DIM = 100
embedding_matrix = np.load('embedding_matrix.npy')


from keras import backend as K
from keras import initializers

class MaxAttentiveMatching(Layer):
    """
       MaxAttentive Matching strategy, each contextual embedding picks the contextual
       embedding of the other sentence with the highest cosine similarity as the
       attentive vector.
    """
    def __init__(self, perspective_num=10, **kwargs):
        self.perspective_num = perspective_num
        self.kernel = None
        super(MaxAttentiveMatching, self).__init__(**kwargs)

    def build(self, input_shape):
        self.dim = input_shape[0][-1]
        self.max_len = input_shape[0][1]
        self.kernel = self.add_weight(name='kernel', shape=(self.perspective_num, self.dim),
                                      initializer='glorot_uniform')
        super(MaxAttentiveMatching, self).build(input_shape)
    
    def _cosine_similarity(self, x1, x2):
        cos = K.sum(x1 * x2, axis=-1)
        x1_norm = K.sqrt(K.maximum(K.sum(K.square(x1), axis=-1), K.epsilon()))
        x2_norm = K.sqrt(K.maximum(K.sum(K.square(x2), axis=-1), K.epsilon()))
        cos = cos / x1_norm / x2_norm
        return cos
    
    def _max_attentive_vectors(self, x2, cosine_matrix):
        """Max attentive vectors.
        Calculate max attentive vector for the entire sentence by picking
        the contextual embedding with the highest cosine similarity
        as the attentive vector.
        # Arguments
            x2: sequence vectors, (batch_size, x2_timesteps, embedding_size)
            cosine_matrix: cosine similarities matrix of x1 and x2,
                           (batch_size, x1_timesteps, x2_timesteps)
        # Output shape
            (batch_size, x1_timesteps, embedding_size)
        """
        # (batch_size, x1_timesteps)
        max_x2_step = K.argmax(cosine_matrix, axis=-1)

        embedding_size = K.int_shape(x2)[-1]
        timesteps = K.int_shape(max_x2_step)[-1]
        if timesteps is None:
            timesteps = K.shape(max_x2_step)[-1]

        # collapse time dimension and batch dimension together
        # collapse x2 to (batch_size * x2_timestep, embedding_size)
        x2 = K.reshape(x2, (-1, embedding_size))
        # collapse max_x2_step to (batch_size * h1_timesteps)
        max_x2_step = K.reshape(max_x2_step, (-1,))
        # (batch_size * x1_timesteps, embedding_size)
        max_x2 = K.gather(x2, max_x2_step)
        # reshape max_x2, (batch_size, x1_timesteps, embedding_size)
        attentive_vector = K.reshape(max_x2, K.stack([-1, timesteps, embedding_size]))
        return attentive_vector
    
    def call(self, inputs, **kwargs):
        sent1 = inputs[0]
        sent2 = inputs[1]

        x1 = K.expand_dims(sent1, axis=2)
        x2 = K.expand_dims(sent2, axis=1)
        cos_matrix  = self._cosine_similarity(x1, x2)

        v1 = K.expand_dims(sent1, -2) * self.kernel
        
        max_attentive_vec = self._max_attentive_vectors(sent2, cos_matrix)
        max_attentive_vec = K.expand_dims(max_attentive_vec, -2) * self.kernel
        matching = self._cosine_similarity(v1, max_attentive_vec)
        return matching

    def compute_output_shape(self, input_shape):
        return input_shape[0][0], input_shape[0][1], self.perspective_num

    def get_config(self):
        config = {'perspective_num': self.perspective_num}
        base_config = super(MaxAttentiveMatching, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


def bilinear_similarity(x1,x2,features_e_dim):
    tmp = Dense(features_e_dim, kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(1e-5))(x1)
    # tmp = Activation("relu")(tmp)
    sim = Dot(-1)([tmp,x2])
    return sim

def get_model(features_e_dim,features_p_dim,features_w_dim):
    enhancers=Input(shape=(MAX_LEN_en,))
    promoters=Input(shape=(MAX_LEN_pr,))
    features_e = Input(shape=(features_e_dim,))
    features_p = Input(shape=(features_p_dim,))
    features_w = Input(shape=(features_w_dim,))
    
    fea_sub = Subtract()([features_e,features_p])
    fea_mul = Multiply()([features_e,features_p])
    
    emb_en=Embedding(NB_WORDS,EMBEDDING_DIM,weights=[embedding_matrix],trainable=True)(enhancers)
    emb_pr=Embedding(NB_WORDS,EMBEDDING_DIM,weights=[embedding_matrix],trainable=True)(promoters)


    enhancer_conv_layer = Conv1D(filters = 64,kernel_size = 40,padding = "valid",kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(1e-5))(emb_en)
    enhancer_conv_layer = Activation("relu")(enhancer_conv_layer)
    enhancer_max_pool_layer = MaxPooling1D(pool_size = 20, strides = 20)(enhancer_conv_layer)
    enh_layer = GlobalMaxPooling1D()(enhancer_conv_layer)
    
    promoter_conv_layer = Conv1D(filters = 64,kernel_size = 40,padding = "valid",kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(1e-5))(emb_pr)
    promoter_conv_layer = Activation("relu")(promoter_conv_layer)
    promoter_max_pool_layer = MaxPooling1D(pool_size = 20, strides = 20)(promoter_conv_layer)  
    pro_layer = GlobalMaxPooling1D()(promoter_conv_layer)
    
    matching_layer = MaxAttentiveMatching(128)
    matching1 = MaxAttentiveMatching(128)([enhancer_max_pool_layer, promoter_max_pool_layer])
    matching2 = MaxAttentiveMatching(128)([promoter_max_pool_layer, enhancer_max_pool_layer])
    matching = Concatenate(axis=1)([matching1, matching2])
    matching = GlobalMaxPooling1D()(matching)
    
    con_1 = Concatenate(axis=1)([enh_layer,matching,pro_layer])

    
    fea_sim = bilinear_similarity(features_e,features_p,features_e_dim) 
    con_2 = Concatenate(axis=1)([fea_sub,fea_sim,fea_mul,features_w])
    con_2 = Dense(128,kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(1e-5))(con_2)
    
    con_3 = Concatenate(axis=1)([con_1,con_2])
    
    
    model_final = Dense(256,kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(1e-5))(con_3)
    model_final = BatchNormalization()(model_final)
    model_final = Activation("relu")(model_final)
    model_final = Dropout(0.5)(model_final)
    preds = Dense(1,kernel_regularizer=regularizers.l2(1e-4))(model_final)
    preds = BatchNormalization()(preds)
    preds = Activation("sigmoid")(preds)

    model=Model([enhancers,promoters,features_e,features_p,features_w],preds)
    adam = Adam(lr=3e-4)
    model.compile(loss='binary_crossentropy',optimizer=adam)
    return model
