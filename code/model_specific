from keras.layers import *
from keras.models import *
from keras.optimizers import Adam
from keras.regularizers import l1, l2
from keras.engine.topology import Layer, InputSpec
from keras import backend as K
from keras import initializers
from keras import activations
from keras import optimizers, regularizers
import numpy as np
MAX_LEN_en = 3000
MAX_LEN_pr = 2000
NB_WORDS = 4097
EMBEDDING_DIM = 100
embedding_matrix = np.load('embedding_matrix.npy')


from keras import backend as K
from keras import initializers

class AttentiveMatching(Layer):
    """
       Attentive Matching strategy, each contextual embedding is compared with its attentive
       weighted representation of the other sentence.
    """
    def __init__(self, perspective_num=10, **kwargs):
        self.perspective_num = perspective_num
        self.kernel = None
        super(AttentiveMatching, self).__init__(**kwargs)

    def build(self, input_shape):
        self.dim = input_shape[0][-1]
        self.max_len = input_shape[0][1]
        self.kernel = self.add_weight(name='kernel', shape=(self.perspective_num, self.dim),
                                      initializer='glorot_uniform')
        super(AttentiveMatching, self).build(input_shape)

    def _cosine_similarity(self, x1, x2):
        cos = K.sum(x1 * x2, axis=-1)
        x1_norm = K.sqrt(K.maximum(K.sum(K.square(x1), axis=-1), K.epsilon()))
        x2_norm = K.sqrt(K.maximum(K.sum(K.square(x2), axis=-1), K.epsilon()))
        cos = cos / x1_norm / x2_norm
        return cos
        
    def _mean_attentive_vectors(self, x2, cosine_matrix):
        """Mean attentive vectors.
        Calculate mean attentive vector for the entire sentence by weighted
        summing all the contextual embeddings of the entire sentence
        # Arguments
            x2: sequence vectors, (batch_size, x2_timesteps, embedding_size)
            cosine_matrix: cosine similarities matrix of x1 and x2,
                           (batch_size, x1_timesteps, x2_timesteps)
        # Output shape
            (batch_size, x1_timesteps, embedding_size)
        """
        # (batch_size, x1_timesteps, x2_timesteps, 1)
        expanded_cosine_matrix = K.expand_dims(cosine_matrix, axis=-1)
        # (batch_size, 1, x2_timesteps, embedding_size)
        x2 = K.expand_dims(x2, axis=1)
        # (batch_size, x1_timesteps, embedding_size)
        weighted_sum = K.sum(expanded_cosine_matrix * x2, axis=2)
        # (batch_size, x1_timesteps, 1)
        sum_cosine = K.expand_dims(K.sum(cosine_matrix, axis=-1) + K.epsilon(), axis=-1)
        # (batch_size, x1_timesteps, embedding_size)
        attentive_vector = weighted_sum / sum_cosine
        return attentive_vector

    def call(self, inputs, **kwargs):
        
        sent1 = inputs[0]
        sent2 = inputs[1]

        x1 = K.expand_dims(sent1, axis=2)
        x2 = K.expand_dims(sent2, axis=1)
        cos_matrix  = self._cosine_similarity(x1, x2)

        v1 = K.expand_dims(sent1, -2) * self.kernel
        
        mean_attentive_vec = self._mean_attentive_vectors(sent2, cos_matrix)
        mean_attentive_vec = K.expand_dims(mean_attentive_vec, -2) * self.kernel
        matching = self._cosine_similarity(v1, mean_attentive_vec)
        return matching

    def compute_output_shape(self, input_shape):
        return input_shape[0][0], input_shape[0][1], self.perspective_num

    def get_config(self):
        config = {'perspective_num': self.perspective_num}
        base_config = super(AttentiveMatching, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


def bilinear_similarity(x1,x2,features_e_dim):
    tmp = Dense(features_e_dim, kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(1e-5))(x1)
    # tmp = Activation("relu")(tmp)
    sim = Dot(-1)([tmp,x2])
    return sim

def get_model(features_e_dim,features_p_dim,features_w_dim):
    enhancers=Input(shape=(MAX_LEN_en,))
    promoters=Input(shape=(MAX_LEN_pr,))
    features_e = Input(shape=(features_e_dim,))
    features_p = Input(shape=(features_p_dim,))
    features_w = Input(shape=(features_w_dim,))
    
    fea_sub = Subtract()([features_e,features_p])
    fea_mul = Multiply()([features_e,features_p])
    
    emb_en=Embedding(NB_WORDS,EMBEDDING_DIM,weights=[embedding_matrix],trainable=True)(enhancers)
    emb_pr=Embedding(NB_WORDS,EMBEDDING_DIM,weights=[embedding_matrix],trainable=True)(promoters)


    enhancer_conv_layer = Conv1D(filters = 64,kernel_size = 40,padding = "valid",kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(1e-5))(emb_en)
    enhancer_conv_layer = Activation("relu")(enhancer_conv_layer)
    enhancer_max_pool_layer = MaxPooling1D(pool_size = 20, strides = 20)(enhancer_conv_layer)
    enh_layer = GlobalMaxPooling1D()(enhancer_conv_layer)
    
    promoter_conv_layer = Conv1D(filters = 64,kernel_size = 40,padding = "valid",kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(1e-5))(emb_pr)
    promoter_conv_layer = Activation("relu")(promoter_conv_layer)
    promoter_max_pool_layer = MaxPooling1D(pool_size = 20, strides = 20)(promoter_conv_layer)  
    pro_layer = GlobalMaxPooling1D()(promoter_conv_layer)
    
    matching1 = AttentiveMatching(128)([enhancer_max_pool_layer, promoter_max_pool_layer])
    matching2 = AttentiveMatching(128)([promoter_max_pool_layer, enhancer_max_pool_layer])
    
    matching1 = Lambda(lambda x: K.permute_dimensions(x, (0, 2, 1)))(matching1)
    matching2 = Lambda(lambda x: K.permute_dimensions(x, (0, 2, 1)))(matching2)
    matching1 = GlobalAveragePooling1D()(matching1)
    matching2 = GlobalAveragePooling1D()(matching2)
    matching = Concatenate(axis=1)([matching1,matching2])
    
    con_1 = Concatenate(axis=1)([enh_layer,matching,pro_layer])

    
    fea_sim = bilinear_similarity(features_e,features_p,features_e_dim) 
    con_2 = Concatenate(axis=1)([fea_sub,fea_sim,fea_mul,features_w])
    con_2 = Dense(128,kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(1e-5))(con_2)
    
    con_3 = Concatenate(axis=1)([con_1,con_2])
    
    
    model_final = Dense(256,kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(1e-5))(con_3)
    model_final = BatchNormalization()(model_final)
    model_final = Activation("relu")(model_final)
    model_final = Dropout(0.5)(model_final)
    preds = Dense(1,kernel_regularizer=regularizers.l2(1e-4))(model_final)
    preds = BatchNormalization()(preds)
    preds = Activation("sigmoid")(preds)

    model=Model([enhancers,promoters,features_e,features_p,features_w],preds)
    adam = Adam(lr=3e-4)
    model.compile(loss='binary_crossentropy',optimizer=adam)
    return model
